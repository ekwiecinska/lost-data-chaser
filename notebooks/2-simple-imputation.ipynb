{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputation of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook will walk through some simple methods of imputing missing data, and explore the effects of these on the distributions of the datasets. The imputation methods covered will include:\n",
    "\n",
    "- Aggregate statistics (mean, median)\n",
    "- Interpolation (time series)\n",
    "- Multiple imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_chaser as dc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again assume that all data is saved in `lost-data-chaser/data/`. There are much nicer ways to do this, but for now let us load these files 'notebook style'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "fnames = sorted([os.path.join(datadir, fname) for fname in os.listdir(datadir) if fname.endswith('.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = pd.read_csv(fnames[0])\n",
    "landslide_df = pd.read_csv(fnames[1])\n",
    "meteor_df = pd.read_csv(fnames[2])\n",
    "comet_df = pd.read_csv(fnames[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df = pd.read_csv(fnames[4], skiprows=39)\n",
    "snow_df['#datetime_MST'] = pd.to_datetime(snow_df['#datetime_MST'])\n",
    "snow_df = snow_df.set_index('#datetime_MST', drop=True)\n",
    "snow_df.index = snow_df.index.set_names('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation via aggregate statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can try imputation using the mean and median of columns. Let us use the fires dataset, imputing the `Velocity Components (km/s): vx` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "fire_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Velocity Components (km/s): vx'\n",
    "fig = go.Figure(\n",
    "    [\n",
    "    go.Bar(x=['Missing', 'Present'], \n",
    "           y=[fire_df[feature].isna().sum(), fire_df[feature].notna().sum()])\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(autosize=True, title_text=f\"Missing data ratios for variable '{feature}'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `SimpleImputer` from `sklearn` to simplify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df[feature + '_mean_impute'] = mean_imputer.fit_transform(fire_df[feature].values.reshape(-1, 1))\n",
    "fire_df[feature + '_median_impute'] = median_imputer.fit_transform(fire_df[feature].values.reshape(-1, 1))\n",
    "print(fire_df[feature + '_mean_impute'])\n",
    "print(fire_df[feature + '_median_impute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wouldn't expect these results to be much different when using them to include more data into a machine learning model,m but we will test this theory in the next notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple time-series imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variable imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (data_chaser)",
   "language": "python",
   "name": "data_chaser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
