{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_chaser as dc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from data_chaser.plot.plotly import missing_value_heatmap, missing_data_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define the data directory. I recommend `lost-data-chaser/data` such that you can follow along with the notebook. The datasets we will use first are all .csv files from:\n",
    "- [Meteorite landings](https://catalog.data.gov/dataset/meteorite-landings)\n",
    "- [Near Earth Comets](https://catalog.data.gov/dataset/near-earth-comets-orbital-elements)\n",
    "- [Fire and Bolide Reports](https://catalog.data.gov/dataset/fireball-and-bolide-reports)\n",
    "- [Global Landslide Catalog](https://catalog.data.gov/dataset/global-landslide-catalog)\n",
    "\n",
    "We will also use an additional time series dataset to try out imputation methods with temporally dependent data.\n",
    "\n",
    "- [SnowEx17 Time Series Sonic Snow Depth Measurement Array](https://nsidc.org/data/SNEX17_SSD/versions/1)\n",
    "\n",
    "To access this dataset alongside the others, follow the instructions in the README.md and extract `SnowEx17_snowdepth_15min_V2.csv` to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "fnames = sorted([os.path.join(datadir, fname) for fname in os.listdir(datadir) if fname.endswith('.csv')])\n",
    "print(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the filenames, let's load the data in and inspect the head to get a feeling of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = pd.read_csv(fnames[0])\n",
    "fire_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslide_df = pd.read_csv(fnames[1])\n",
    "landslide_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_df = pd.read_csv(fnames[1])\n",
    "meteor_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_df = pd.read_csv(fnames[3])\n",
    "comet_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the snow data, the first 39 rows contain the header, so remove this when reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df = pd.read_csv(fnames[4], skiprows=39)\n",
    "snow_df['#datetime_MST'] = pd.to_datetime(snow_df['#datetime_MST'])\n",
    "snow_df = snow_df.set_index('#datetime_MST', drop=True)\n",
    "snow_df.index = snow_df.index.set_names('timestamp')\n",
    "snow_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the `NaN` distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, for this challenge we don't *really* mind about the semantic value of each column e.g what does 'P (yr) mean? For building a general case strategy, we onluy care about the type of data (categorical, continuous...) and the sparsity. Therefore we will skimp on any EDA that is **too** in depth into what the data 'means'. Unusual for a data scientist. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of NaNs in each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start implementing a solution, it is important for us to visualise the distribution of missing values (or `NaNs`) for each dataset. This way, we can better understand the sparsity of the data that we're dealing with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = missing_value_heatmap(fire_df, \"fire_df\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = missing_value_heatmap(meteor_df, \"meteor_df\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = missing_value_heatmap(landslide_df, \"landslide_df\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = missing_value_heatmap(comet_df, \"comet_df\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = missing_value_heatmap(snow_df, \"snow_df\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, we can see there are generally 4 types of missing data challenges that we must consider: \n",
    "1. Columns with **complete** sparsity (no values)\n",
    "2. Columns with **high** sparsity (around 90% of values are missing)\n",
    "3. Columns with **low/medium sparsity** (50% or higher values are present)\n",
    "4. Completely sparsity (few values in most columns). This type isn't present in these datasets but we can experiment with engineering some.\n",
    "\n",
    "We must also consider dependencies (or lack of) in the data. Some columns may be measuring samples (rows) with some temporal dependence on each other, e,g a time series from the same signal. Others may be measuring **independent** events. This is important to consider when designing an imputation strategy in which values you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise NaNs in a time series dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's plot the time series data with gaps in for missing values. :) From this plot, we can see that sensor `SXK5` has many missing values (double click on `SXK5` in the legend to only see its signal!). Have a look at this in comparison to a sensor such as `SXN4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = time_series_with_nans(snow_df, 'depth_qc_cm', 'sensor_num')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio of missing data to present data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_fig = missing_data_ratios([comet_df, meteor_df, landslide_df, fire_df, snow_df], ['comet_df', 'meteor_df', 'landslide_df', 'fire_df', 'snow_df'])\n",
    "ratio_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explored the datasets, we can move on to preprocessing them to prepare them for an imputation strategy in our next notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (data_chaser)",
   "language": "python",
   "name": "data_chaser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
